#!/usr/bin/env python3
"""
Ø³ÙƒØ±ÙŠØ¨Øª ØªØ±Ø¬Ù…Ø© ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù…Ù† Ù…Ù„ÙØ§Øª JSX ÙˆÙŠØªØ±Ø¬Ù…Ù‡Ø§ Ù„Ù€ 4 Ù„ØºØ§Øª
"""

import re
import json
import os
from pathlib import Path

# ============================================
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# ============================================

# Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
PAGES_DIR = "src/pages"
OUTPUT_FILE = "translations_extracted.json"

# Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†ØµÙˆØµ
PATTERNS = {
    # Ù†ØµÙˆØµ Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ Ù…Ø²Ø¯ÙˆØ¬Ø©
    'double_quotes': r'"([^"]{3,})"',
    # Ù†ØµÙˆØµ Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ Ù…ÙØ±Ø¯Ø©  
    'single_quotes': r"'([^']{3,})'",
    # Ù†ØµÙˆØµ ÙÙŠ JSX
    'jsx_text': r'>\s*([^<>{}\n]{3,})\s*<',
}

# Ø£Ù†Ù…Ø§Ø· Ù„ØªØ¬Ø§Ù‡Ù„Ù‡Ø§
IGNORE_PATTERNS = [
    r'^[a-zA-Z0-9_\-\.\/]+$',  # Ø£Ø³Ù…Ø§Ø¡ Ù…Ù„ÙØ§Øª ÙˆÙ…ØªØºÙŠØ±Ø§Øª
    r'^className$',
    r'^onClick$',
    r'^onChange$',
    r'^\d+$',  # Ø£Ø±Ù‚Ø§Ù… ÙÙ‚Ø·
    r'^[a-z]+\.[a-z]+$',  # Ù…Ø«Ù„ item.title
    r'^t\(',  # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª t() Ù…ÙˆØ¬ÙˆØ¯Ø©
    r'^\$',  # Ù…ØªØºÙŠØ±Ø§Øª
    r'^https?://',  # Ø±ÙˆØ§Ø¨Ø·
]

# ============================================
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# ============================================

def is_arabic(text):
    """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø±Ø¨ÙŠ"""
    arabic_pattern = re.compile(r'[\u0600-\u06FF]')
    return bool(arabic_pattern.search(text))

def is_chinese(text):
    """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙŠÙ†ÙŠ"""
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
    return bool(chinese_pattern.search(text))

def should_ignore(text):
    """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ¬Ø¨ ØªØ¬Ø§Ù‡Ù„Ù‡"""
    text = text.strip()
    
    # Ù†ØµÙˆØµ Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
    if len(text) < 3:
        return True
    
    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¬Ø§Ù‡Ù„Ø©
    for pattern in IGNORE_PATTERNS:
        if re.match(pattern, text):
            return True
    
    return False

def clean_text(text):
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ"""
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    text = ' '.join(text.split())
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©
    text = text.strip('.,;:!?()[]{}\'\"')
    return text

def extract_texts_from_file(file_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„Ù JSX"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {file_path}: {e}")
        return []
    
    texts = set()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    for pattern_name, pattern in PATTERNS.items():
        matches = re.findall(pattern, content)
        for match in matches:
            text = clean_text(match)
            
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
            if should_ignore(text):
                continue
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙÙ‚Ø·
            if is_arabic(text) or (len(text) > 5 and text[0].isupper()):
                texts.add(text)
    
    return list(texts)

def generate_translation_key(text, existing_keys):
    """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ ØªØ±Ø¬Ù…Ø© ÙØ±ÙŠØ¯"""
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
    key = re.sub(r'[^\w\s]', '', text.lower())
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¨Ù€ _
    key = '_'.join(key.split())
    # ØªÙ‚ØµÙŠØ± Ø¥Ù„Ù‰ 50 Ø­Ø±Ù
    key = key[:50]
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙƒØ±Ø§Ø±
    original_key = key
    counter = 1
    while key in existing_keys:
        key = f"{original_key}_{counter}"
        counter += 1
    
    return key

def categorize_text(text, file_name):
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù†Øµ"""
    file_lower = file_name.lower()
    
    # Ø­Ø³Ø¨ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
    if 'home' in file_lower:
        return 'home'
    elif 'repentance' in file_lower or 'tawba' in file_lower:
        return 'repentance'
    elif 'fatwa' in file_lower:
        return 'fatwa'
    elif 'learn' in file_lower or 'islam' in file_lower:
        return 'learn_islam'
    elif 'contact' in file_lower:
        return 'contact'
    elif 'course' in file_lower:
        return 'courses'
    elif 'profile' in file_lower:
        return 'profile'
    elif 'reconciliation' in file_lower:
        return 'reconciliation'
    
    # Ø­Ø³Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Øµ
    text_lower = text.lower()
    if any(word in text_lower for word in ['login', 'register', 'password', 'email']):
        return 'auth'
    elif any(word in text_lower for word in ['search', 'filter', 'find']):
        return 'search'
    elif any(word in text_lower for word in ['save', 'delete', 'edit', 'cancel']):
        return 'common'
    
    return 'common'

# ============================================
# Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ============================================

def extract_all_texts(pages_directory):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª"""
    
    if not os.path.exists(pages_directory):
        print(f"âŒ Ø§Ù„Ù…Ø¬Ù„Ø¯ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {pages_directory}")
        return {}
    
    all_texts = {}
    existing_keys = set()
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª JSX
    jsx_files = list(Path(pages_directory).rglob('*.jsx'))
    
    print(f"\nğŸ” Ø¬Ø§Ø±ÙŠ ÙØ­Øµ {len(jsx_files)} Ù…Ù„Ù...\n")
    
    for file_path in jsx_files:
        file_name = file_path.stem
        print(f"ğŸ“„ {file_name}.jsx")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
        texts = extract_texts_from_file(file_path)
        
        if not texts:
            print(f"   âš ï¸  Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØµÙˆØµ\n")
            continue
        
        print(f"   âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(texts)} Ù†Øµ\n")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù†Øµ
        for text in texts:
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø©
            category = categorize_text(text, file_name)
            
            # ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­
            key = generate_translation_key(text, existing_keys)
            existing_keys.add(key)
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù†ØªØ§Ø¦Ø¬
            if category not in all_texts:
                all_texts[category] = {}
            
            all_texts[category][key] = {
                'ar': text if is_arabic(text) else '',
                'en': text if not is_arabic(text) else '',
                'fr': '',
                'zh': '',
                'source_file': file_name,
                'needs_translation': True
            }
    
    return all_texts

def save_results(data, output_file):
    """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù JSON"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_file}")

def print_statistics(data):
    """Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
    total_texts = sum(len(category) for category in data.values())
    arabic_texts = sum(
        1 for category in data.values() 
        for item in category.values() 
        if item['ar']
    )
    english_texts = sum(
        1 for category in data.values() 
        for item in category.values() 
        if item['en']
    )
    
    print("\n" + "="*50)
    print("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
    print("="*50)
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØµÙˆØµ: {total_texts}")
    print(f"Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ©: {arabic_texts}")
    print(f"Ù†ØµÙˆØµ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: {english_texts}")
    print(f"\nØ§Ù„ÙØ¦Ø§Øª:")
    for category, items in data.items():
        print(f"  â€¢ {category}: {len(items)} Ù†Øµ")
    print("="*50)

# ============================================
# Ø§Ù„ØªØ´ØºÙŠÙ„
# ============================================

if __name__ == "__main__":
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„ÙØ§Øª JSX...\n")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
    extracted_data = extract_all_texts(PAGES_DIR)
    
    if not extracted_data:
        print("\nâŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù†ØµÙˆØµ!")
    else:
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        save_results(extracted_data, OUTPUT_FILE)
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        print_statistics(extracted_data)
        
        print(f"\nâœ¨ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡!")
        print(f"\nğŸ“ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:")
        print(f"   1. Ø±Ø§Ø¬Ø¹ Ù…Ù„Ù {OUTPUT_FILE}")
        print(f"   2. Ø´ØºÙ‘Ù„ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ±Ø¬Ù…Ø©: python translate_texts.py")
